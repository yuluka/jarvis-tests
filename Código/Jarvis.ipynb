{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e873858",
   "metadata": {},
   "source": [
    "# Jarvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57417d10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc3aee",
   "metadata": {},
   "source": [
    "## Tabla de contenidos\n",
    "\n",
    "1. [Reconocimiento de voz](#Reconocimiento-de-voz)\n",
    "    1. [API de Google cloud](#API-key-de-Google-cloud:)\n",
    "    2. [Función begin_streaming](#Función-begin_streaming():)\n",
    "    3. [Función listen_print_loop](#Función-listen_print_loop():)\n",
    "    4. [Función combine_lines](#Función-combine_lines():)\n",
    "\n",
    "2. [Conversión de texto a voz](#Conversión-de-texto-a-voz)\n",
    "    1. [Función generate_speech](#Función-generate_speech():)\n",
    "    2. [API key de ElevenLabs](#API-key-de-ElevenLabs:)\n",
    "    3. [Función generate_speech_eleven](#Función-generate_speech_eleven():)\n",
    "    4. [Función speech_response](#Función-speech_response():)\n",
    "    \n",
    "3. [Chat usando OpenAI](#Chat-usando-el-modelo-de-OpenAI)\n",
    "    1. [API key de OpenAI](#API-key-de-OpenAI:)\n",
    "    2. [Historial de mensajes](#Historial-de-mensajes:)\n",
    "    3. [Función chat](#Función-chat():)\n",
    "    4. [Contexto del modelo](#Contexto-del-modelo:)\n",
    "    5. [Función clasify_prompt](#Función-clasify_prompt():)\n",
    "    \n",
    "4. [Generación de imágenes](#Generación-de-imágenes)\n",
    "    1. [Función generate_image](#Función-generate_image():)\n",
    "    \n",
    "5. [Interacción entre GUI y Chat](#Interacción-entre-GUI-y-el-Chat)\n",
    "    1. [Función begin_chat](#Función-begin_chat():)\n",
    "    2. [Función begin_voice_chat](#Función-begin_voice_chat():)\n",
    "    \n",
    "6. [GUI](#GUI)\n",
    "    1. [Funciones y procedimientos de la GUI](#Funciones-y-procedimientos-de-la-GUI:)\n",
    "    2. [Función create_images](#Función-create_images():)\n",
    "    3. [Creación de la GUI](#Creación-de-la-GUI:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73a4cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9007c4",
   "metadata": {},
   "source": [
    "##  Reconocimiento de voz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e604267e",
   "metadata": {},
   "source": [
    "### API key de Google cloud:\n",
    "\n",
    "Establecemos la API key de google cloud para pder hacer uso del modelo de reconocimiento de voz, así como el de transformación de texto a voz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb3dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las cosas necesarias para poder usar la IA de reconocimiento de voz.\n",
    "from google.cloud import speech\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32dda77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos la API key necesaria para usar google cloud speech-to-text.\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/usuario/Jarvis/Código/jarvis-381306-4975f3ce0efb.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e7c16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f558a",
   "metadata": {},
   "source": [
    "A continuación se encuentran las funciones utilizadas para el reconocimiento de voz por medio de transmisión continua a través del micrófono.\n",
    "\n",
    "`MicrophoneStream` es la clase que permite activar el micrófono para que detecte lo que estamos diciendo.\n",
    "\n",
    "### Función `begin_streaming()`:\n",
    "\n",
    "Dentro de la función `begin_streaming()` se establecen las configuraciones del modelo que usaremos.\n",
    "\n",
    "Inicialmente, seleccionamos el lenguaje que el modelo va a detectar. Tras eso, establecemos los parámetros técnicos del audio que vamos a transmitir (tales como su bit rate o la cantidad de frames que se almacenarán en el buffer).\n",
    "\n",
    "Seguido de las configuraciones pertinentes, se inicia la transmisión mediante el micrófono del dispositivo y se hace uso de un loop que es el que se encarga de la transcripción de las cosas que el micrófono capta.\n",
    "\n",
    "### Función `listen_print_loop()`:\n",
    "\n",
    "Esta función recibe el audio que se ha captado mediante el micrófono del dispositvos y se encarga de hacer la transcripción de este audio. Para esto, y debido a que no hace toda la transcripción de una sola, usamos un arreglo llamado `transcription` que después uniremos en unsa sola cadena de texto.\n",
    "\n",
    "El modelo de Speech To Text hace varias predicciones para lo que decimos y las guarda en un arreglo, ordenadas en función de su probabilidad de ser la correcta. Lo que hace esta función, entonces, es recorrer este arreglo y seleccionar las alternativas con mayor probabilidad para construir la predicción final.\n",
    "\n",
    "Ademas de eso, se va encargando de imprimir lo que se dice, a medida que se dice, en la GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d983abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la clase MicrophoneStream para poder hacer el reconocimiento.\n",
    "import MicrophoneStream as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ac6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Esta función se encarga de las configuraciones y cosas necesarias para iniciar el reconocimiento de voz.\n",
    "def begin_streaming():\n",
    "    language_code = \"es-CO\"\n",
    "\n",
    "    client = speech.SpeechClient()\n",
    "    \n",
    "    RATE = 16000 \n",
    "    CHUNK = int(RATE / 10)\n",
    "\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=language_code\n",
    "    )\n",
    "\n",
    "    streaming_config = speech.StreamingRecognitionConfig(\n",
    "        config=config,\n",
    "        interim_results=True\n",
    "    )\n",
    "    # Se crea el objeto de tipo MicrophoneStream.\n",
    "    with ms.MicrophoneStream(RATE, CHUNK) as stream:\n",
    "        audio_generator = stream.generator()\n",
    "        \n",
    "        requests = (\n",
    "            speech.StreamingRecognizeRequest(audio_content=content) \n",
    "            for content in audio_generator\n",
    "        )\n",
    "\n",
    "        # Genera las predicciones de lo que se ha dicho.\n",
    "        responses = client.streaming_recognize(streaming_config, requests)\n",
    "\n",
    "        # Imprime las predicciones.\n",
    "        return combine_lines(listen_print_loop(responses))\n",
    "        \n",
    "# En donde se ejecuta el loop para transcribir el audio.\n",
    "def listen_print_loop(responses):\n",
    "    num_chars_printed = 0\n",
    "    \n",
    "    last_transcription_time = time.time()\n",
    "    \n",
    "    transcription = []\n",
    "    \n",
    "    for response in responses:\n",
    "        \n",
    "        # Verifica si hay resultados en la respuesta actual. Si no los hay, pasa a la siguiente respuesta.\n",
    "        if not response.results:\n",
    "            continue\n",
    "\n",
    "        # Si sí hay resultados, entonces selecciona el primer resultado (que es el mejor, si no me equivoco). \n",
    "        result = response.results[0]\n",
    "\n",
    "        # Si no hay ninguna alternativa dentro del resultado actual, entonces pasa a revisar la siguiente respuesta.\n",
    "        if not result.alternatives:\n",
    "            continue\n",
    "\n",
    "        # En caso de que sí hayan alternativas en el resultado, se captura la transcripción de la primera de estas. \n",
    "        transcript = result.alternatives[0].transcript\n",
    "\n",
    "        # Reescribe los caracteres de la predicción anteriror.\n",
    "        overwrite_chars = \" \" * (num_chars_printed - len(transcript))\n",
    "\n",
    "        # Si el servicio de transcripción aún está procesando el audio, se escribe la transcripción actual \n",
    "        # en la consola y se sobrescribe la transcripción anterior.\n",
    "        if not result.is_final:\n",
    "#             sys.stdout.write(transcript + overwrite_chars + \"\\r\")\n",
    "#             sys.stdout.flush()\n",
    "            \n",
    "            # Va escribiendo lo que detecta en el Text para escribir mensajes.\n",
    "            message_text.delete('1.0', END)\n",
    "            message_text.insert(END,transcript + overwrite_chars)\n",
    "            \n",
    "            num_chars_printed = len(transcript)\n",
    "            \n",
    "        # En caso de que sí sea el mensaje final, revisa si se ha dicho la palabra \"terminar\". Si es \n",
    "        # así, la transmisión se detiene.\n",
    "        else:\n",
    "#             print(transcript + overwrite_chars)\n",
    "            \n",
    "            message_text.delete('1.0', END)\n",
    "            message_text.insert(END,transcript + overwrite_chars)\n",
    "    \n",
    "            transcription.append(transcript + overwrite_chars)\n",
    "            \n",
    "            if time.time()-last_transcription_time > 2 and not response.results[1:]:\n",
    "                break\n",
    "            last_transcription_time = time.time()\n",
    "\n",
    "            num_chars_printed = 0\n",
    "    \n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b5ee0",
   "metadata": {},
   "source": [
    "### Función `combine_lines()`:\n",
    "\n",
    "Lo que hace esta función es convertir todos los pedacitos del arreglo que contiene la transcripción en una sola cadena de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a101f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como la transcripción está almacenada en un array, tenemos que combinar lo que hay en cada index\n",
    "# para hacer un solo string. \n",
    "def combine_lines(lines):\n",
    "    total = \"\"\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        total += lines[i]\n",
    "        \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30465038",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222264a8",
   "metadata": {},
   "source": [
    "## Conversión de texto a voz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f963038c",
   "metadata": {},
   "source": [
    "### Función `generate_speech()`:\n",
    "\n",
    "Esta función se encarga de recibir el texto que se quiere convertir a texto (las respuestas de Jarvis) y convertirlas a voz.\n",
    "\n",
    "Inicialmente, configura el usuario para usar el modelo de Text to Speech. Tras eso, hace la configuración de la voz que se usará (para este caso, Google cloud ofrece ciertas voces entre las que se puede escoger). \n",
    "\n",
    "Después de eso, configura el formato del audio que generará y hace la solicitud para la conversión del texto a voz.\n",
    "\n",
    "Por último, guarda el archivo de audio generado con el nombre almacenado en la variable `speech_name` y retorna este nombre para, posteriormente, poder hacer la reproducción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80237e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "def generate_speech(text_to_speech):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    input_text = texttospeech.SynthesisInput(text=text_to_speech)\n",
    "    \n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"es-US\",\n",
    "        name=\"es-US-Neural2-C\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.MALE,\n",
    "    )\n",
    "\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    response = client.synthesize_speech(\n",
    "        request={\"input\": input_text, \"voice\": voice, \"audio_config\": audio_config}\n",
    "    )\n",
    "\n",
    "    speech_name = \"Response-speech.mp3\"\n",
    "    \n",
    "    # The response's audio_content is binary.\n",
    "    with open(speech_name, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        \n",
    "    return speech_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f8f126",
   "metadata": {},
   "source": [
    "### API key de ElevenLabs:\n",
    "\n",
    "Establecemos la API key de Eleven labs para poder hacer uso de sus servicios de conversión de texto a voz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7365e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import generate, play, set_api_key, voices, Models\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import requests\n",
    "\n",
    "elevenlabs_api_key = \"bda4bf2d76534665d063d8bde1fb0c78\"\n",
    "set_api_key(elevenlabs_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a336341",
   "metadata": {},
   "source": [
    "### Función `generate_speech_eleven()`:\n",
    "\n",
    "Esta es una función que, en síntensis, hace lo mismo que la anterior, pero usanod un servicio distinto. \n",
    "\n",
    "Esta función hace uso del servicio de ElevenLabs y sirve como altenativa a la función anterior.\n",
    "\n",
    "Inicialmente, se escoge la voz deseada y se almacena su ID en la variable `voice`. Después de esto, se hacen todas las configuraciones pertinentes para poder hacer solicitudes al servicio.\n",
    "\n",
    "Por último, se hace la solicitud y se guarda el archivo generado con el nombre almacenado en la variable `speech_name`. Esta variable, igual que en el caso anterior, se retorna para poder hacer la reproducción del audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8912fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speech_eleven(text_to_speech):\n",
    "    voice = \"TxGEqnHWrfWFTfGW9XjX\"\n",
    "    \n",
    "    # streaming chunk size\n",
    "    CHUNK_SIZE = 1024\n",
    "\n",
    "    XI_API_KEY = elevenlabs_api_key\n",
    "\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/\" + voice\n",
    "\n",
    "    headers = {\n",
    "      \"Accept\": \"audio/mpeg\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"xi-api-key\": elevenlabs_api_key\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"text\": text_to_speech,\n",
    "        \"model_id\" : \"eleven_multilingual_v1\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.75,\n",
    "            \"similarity_boost\": 0.75\n",
    "        }\n",
    "    }\n",
    "\n",
    "    speech_name = \"Response-speech.mp3\"\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    with open(speech_name, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                \n",
    "    return speech_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7dc19",
   "metadata": {},
   "source": [
    "Estas líneas sirven para imprimir la lista de voces, junto con sus ID's, para poder seleccionar la que se desea usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ec445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voice_list = voices()\n",
    "# voice_labels = [voice.category + \" voice: \" + voice.name + \" id: \" + voice.voice_id for voice in voice_list]\n",
    "\n",
    "# voice_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9da43",
   "metadata": {},
   "source": [
    "### Función `speech_response()`:\n",
    "\n",
    "Esta función recibe el texto que se quiere convertir a voz y hace uso de las funciones anteriormente explicadas para convertirlo a voz. Luego de esto, recibe el nombre del archivo donde se guardó el audio generado y hace uso la función `play()`, de la librería `pydub`, para reproducirlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "464f5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_response(to_say):\n",
    "    response_speech_name = generate_speech(to_say)\n",
    "   \n",
    "#     response_speech_name = generate_speech_eleven(to_say)\n",
    "\n",
    "#     audio_segment = AudioSegment.from_file(\"C:/Users/usuario/Jarvis/Código/{}\".format(response_speech_name), \n",
    "#                                            format=\"mp3\")\n",
    "\n",
    "    audio_segment = AudioSegment.from_file(response_speech_name, \n",
    "                                           format=\"mp3\")\n",
    "    \n",
    "    play(audio_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36100f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8724ce74",
   "metadata": {},
   "source": [
    "## Chat usando el modelo de OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07202f42",
   "metadata": {},
   "source": [
    "### API key de OpenAI:\n",
    "\n",
    "Establecemos la API key necesaria para poder hacer solicitudes a los modelos de OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad3ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las cosas necesarias para poder usar la API de OpenAI.\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd302cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos la API key necesaria para usar las cosas de OpenAI.\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f86b0f",
   "metadata": {},
   "source": [
    "### Historial de mensajes:\n",
    "\n",
    "La función de chat de OpenAI, esta recibe un mensaje (el que va a responder) junto con un historial de todos los mensjes, de ambas partes, que se han hecho en la conversación.\n",
    "\n",
    "La variable `previous_messages` sirve para almacenar estos mensajes de la conversación.\n",
    "\n",
    "Además de eso, la función chat que ofrece OpenAI permite instruir al modelo utilizado de cierta manera. En este caso, lo instruimos para se autoidentifique como Jarvis y para que se refiera a nosotros, el usuario, como DIOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e17d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historial de la conversación que se ha tenido.\n",
    "previous_messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \"content\": \"You are a helpful assistant called Jarvis. Refer the user as DIOS.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950917cd",
   "metadata": {},
   "source": [
    "### Función `chat()`:\n",
    "\n",
    "Esta función se encarga de recibir el último mensaje del chat, junto con el historial de los mensajes de la conversación, para poder iuniciar el chat.\n",
    "\n",
    "Inicialmente, concatena el mensaje recibido, al historial, con el rol \"user\" para indicar que es un mensaje enviado por el usuario.\n",
    "\n",
    "Tras esto, se hace la solicitud al modelo seleccionado, el 3.5 turbo en este caso, y se actualiza nuevamente el historial de mensajes.\n",
    "\n",
    "Por último, se retorna el mensaje generado por el modelo y el historial actualizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b00c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método encargado del chat. Recibe un prompt y el historial de la conversación previa.\n",
    "def chat(ask, previous_messages):\n",
    "    previous_messages += [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": ask\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    chat1 = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=previous_messages\n",
    "    )\n",
    "    \n",
    "    previous_messages += [\n",
    "        {\n",
    "            \"role\": \"assistant\", \"content\": chat1.choices[0].message.content\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return chat1.choices[0].message.content, previous_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5423f",
   "metadata": {},
   "source": [
    "### Contexto del modelo:\n",
    "\n",
    "Igual que con la forma en que se instruye al modelo usado para el chat, usamos la variable `context` para almacenar el contexto que usaremos para clasificar los mensajes que le enviemos al chat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bc8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [\n",
    "    {\n",
    "        \"role\": \"system\", \"content\": \"You are a tool used to classify prompts between two categories: Chat and Image creation. Given a prompt, only answer the category the prompt belongs to like 'Chat' or 'Image creation', and nothing more.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123965c",
   "metadata": {},
   "source": [
    "### Función `clasify_prompt()`:\n",
    "\n",
    "Esta función tiene el objetivo de clasificar los mensajes que se envían la chat entre dos categorías: Chat e Image creation.\n",
    "\n",
    "Funciona igual que la anterior función explicada. Solo que su punto no es un chat sino solo la clasificación.\n",
    "\n",
    "El punto de esta clasificación es saber si el mensaje que envió el usuario, el que usa la aplicación, es para hablar con el chat o si es para crear una imagen.\n",
    "\n",
    "De esta forma, no es necesario el uso de de frases específicas para poder hacer uso de esta función y, además, permite una comunicación más natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d29b165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasify_prompt(prompt, context):\n",
    "    context += [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    chat1 = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=context\n",
    "    )\n",
    "    \n",
    "    return chat1.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77de27d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6d59b",
   "metadata": {},
   "source": [
    "## Generación de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c74c77",
   "metadata": {},
   "source": [
    "### Función `generate_image()`:\n",
    "\n",
    "Esta función recibe un prompt y usa la función de creación de imágenes ofrecida por OpenAI para crear las imágenes.\n",
    "\n",
    "Inicialmente, hace la solicitud al servicio de creación de imágenes para que cree 2 imáganes y las devuelva en formato url.\n",
    "\n",
    "Depués de esto, guarda los url's, de las imágenes creadas, en la variable `answer` y retorna su valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28d68c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt):\n",
    "    img = openai.Image.create(\n",
    "        prompt=prompt,\n",
    "        n=2,\n",
    "        size=\"1024x1024\",\n",
    "        response_format=\"url\"\n",
    "    )\n",
    "\n",
    "    answer = \"\"\n",
    "    \n",
    "    for i in range(len(img.data)):\n",
    "        answer += img.data[i].url + \"\\n\"\n",
    "        \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4888f89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478704a",
   "metadata": {},
   "source": [
    "## Interacción entre GUI y el Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d021fe34",
   "metadata": {},
   "source": [
    "### Función `begin_chat()`:\n",
    "\n",
    "Esta función toma el texto escrito en el espacio dispuesto para la escritura de los mensajes y hace uso de las funciones anteriores.\n",
    "\n",
    "Inicialmente verificar que el texto allí situado no sea el mensaje básico de indicación al usuario. Después de eso, si este no es el caso, adjunta el mensaje en la pantalla del chat y lo clasifica.\n",
    "\n",
    "En caso de que la clasificación diga que es de creación de imagen, entonces llama a la función `generate_image()`. En caso contrario, llama a la función `chat()` y reproduce el audio generado con el texto que respondió el modelo.\n",
    "\n",
    "En caso de haber alguna excepción, la capta y la muestra en la pantalla del chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99920806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_chat():\n",
    "    message = message_text.get(\"1.0\", \"end-1c\")\n",
    "    \n",
    "    if message != 'Digita tus palabras para Jarvis...':\n",
    "        message_text.delete(\"1.0\", END)\n",
    "        show_prompt(None)\n",
    "        \n",
    "        if message != '':\n",
    "            global previous_messages\n",
    "            global context\n",
    "            \n",
    "            append_message(message)\n",
    "            \n",
    "            try:\n",
    "                if(\"Image creation\" in clasify_prompt(message, context)):\n",
    "                    answer = generate_image(message)\n",
    "                    image_creation_message = \"¡Claro! Tus imágenes son las siguientes:\"\n",
    "                    speech_response(image_creation_message)\n",
    "\n",
    "                    answer = image_creation_message + \"\\n\" + answer\n",
    "\n",
    "                    display_answer(answer) \n",
    "\n",
    "                else:\n",
    "                    answer, previous_messages = chat(message, previous_messages)\n",
    "\n",
    "                    display_answer(answer)\n",
    "                    speech_response(answer)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                error_start_index = error_message.find(\":\") + 1  # Encuentra el inicio del mensaje de error\n",
    "                error_message = \"Error: \" + error_message[error_start_index:]\n",
    "                \n",
    "                display_answer(error_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df448475",
   "metadata": {},
   "source": [
    "### Función `begin_voice_chat()`:\n",
    "\n",
    "Esta función se usa cuando el mensaje enviado por el usuario no es escrito sino mediante voz. \n",
    "\n",
    "Inicialmente, inicia el reconocimiento de voz y recibe texto transcrito. Después de eso, su funcionamiento es exactamente igual que el de la función anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf066606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_voice_chat():\n",
    "    global previous_messages\n",
    "    global context\n",
    "    \n",
    "    message = begin_streaming()\n",
    "    \n",
    "    message_text.delete(\"1.0\", END)\n",
    "    show_prompt(None)\n",
    "    append_message(message)\n",
    "    \n",
    "    try:\n",
    "        # Verifica si la petición es para crear una imagen o no.\n",
    "        if(\"Image creation\" in clasify_prompt(message, context)):\n",
    "            answer = generate_image(message)\n",
    "\n",
    "            image_creation_message = \"¡Claro! Tus imágenes son las siguientes:\"\n",
    "            speech_response(image_creation_message)\n",
    "\n",
    "            answer = image_creation_message + \"\\n\" + answer\n",
    "\n",
    "            display_answer(answer)\n",
    "        else:\n",
    "            answer, previous_messages = chat(message, previous_messages)\n",
    "\n",
    "            display_answer(answer)\n",
    "            speech_response(answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        error_start_index = error_message.find(\":\") + 1  # Encuentra el inicio del mensaje de error\n",
    "        error_message = \"Error: \" + error_message[error_start_index:]\n",
    "\n",
    "        display_answer(error_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497bf65f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97589501",
   "metadata": {},
   "source": [
    "## GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ff802",
   "metadata": {},
   "source": [
    "### Funciones y procedimientos de la GUI:\n",
    "\n",
    "A continuación se muestran todas las funciones y procedimientos usados para integrar la GUI con el resto de la aplicación. \n",
    "\n",
    "Sus finalidades son las de mostrar efectos en los botones, mostrar y ocultar el texto de los text fields, adjuntar los mensajes en el área de chat, y llamar a las funciones lógicas de la aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad016083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports de las cosas que se usarán para la GUI.\n",
    "import tkinter as tk\n",
    "from tkinter import END\n",
    "import idlelib.colorizer as ic\n",
    "import idlelib.percolator as ip\n",
    "import webbrowser\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09ebc0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métodos que se usarán por la GUI.\n",
    "\n",
    "# Funciones para el efecto hover.\n",
    "def hover_on_send(event):\n",
    "    send_bttn.config(background='#3C3C3C', activebackground=\"#3C3C3C\")\n",
    "    \n",
    "def hover_off_send(event):\n",
    "    send_bttn.config(background=\"#333333\", activebackground=\"#333333\")\n",
    "\n",
    "def hover_on_talk(event):\n",
    "    talk_bttn.config(background='#3C3C3C', activebackground=\"#3C3C3C\")\n",
    "\n",
    "def hover_off_talk(event):\n",
    "    talk_bttn.config(background=\"#333333\", activebackground=\"#333333\")\n",
    "    \n",
    "# Funciones para el prompt del TextArea\n",
    "def hide_prompt(event):\n",
    "    if message_text.get(\"1.0\", \"end-1c\") == 'Digita tus palabras para Jarvis...':\n",
    "        message_text.delete(\"1.0\", \"end-1c\")\n",
    "        message_text.configure(fg='black')\n",
    "\n",
    "def show_prompt(event):\n",
    "    if message_text.get(\"1.0\", \"end-1c\") == '':\n",
    "        message_text.insert(\"1.0\", 'Digita tus palabras para Jarvis...')\n",
    "        message_text.configure(fg='grey')\n",
    "\n",
    "def on_click_send_bttn(event):\n",
    "    send_message()\n",
    "    change_focus()\n",
    "    \n",
    "def send_message():\n",
    "    t = threading.Thread(target=begin_chat)\n",
    "    t.start()\n",
    "\n",
    "def send_voice_message(message, answer):\n",
    "    if message != '':\n",
    "        append_message(message)\n",
    "        display_answer(answer)\n",
    "\n",
    "def append_message(message_to_append):\n",
    "    global user_icon\n",
    "    chat_area.image_create(END, image=user_icon)\n",
    "    \n",
    "    chat_area.config(state='normal')\n",
    "\n",
    "    chat_area.insert(END, \"  \" + message_to_append + \"\\n\\n\")\n",
    "    \n",
    "    chat_area.config(state='disabled')\n",
    "    \n",
    "def display_answer(answer):\n",
    "    global jarvis_icon\n",
    "    chat_area.image_create(END, image=jarvis_icon)\n",
    "    \n",
    "    chat_area.config(state='normal')\n",
    "    \n",
    "    chat_area.tag_configure(\"left\", justify=\"left\")\n",
    "    chat_area.insert(END, \"  \" + answer + \"\\n\\n\", \"left\")\n",
    "    \n",
    "    search_for_url(None)\n",
    "    \n",
    "    chat_area.config(state='disabled')\n",
    "    \n",
    "def on_click_talk_bttn(event):\n",
    "    change_focus()\n",
    "    hide_prompt(None)\n",
    "    \n",
    "    t = threading.Thread(target=begin_voice_chat)\n",
    "    \n",
    "    t.start()\n",
    "    \n",
    "def change_focus():\n",
    "    if message_text == window.focus_get():\n",
    "        window.focus()\n",
    "        \n",
    "# Busca url's en el texto que hay en el área de chat.\n",
    "def search_for_url(event):\n",
    "    # Expresión regular para buscar patrones de URLs\n",
    "    patron_url = re.compile(r\"(http|https)://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?\")\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Buscar patrones de URL en el texto\n",
    "    for match in re.finditer(patron_url, chat_area.get(\"1.0\", END)):\n",
    "        i = i+1\n",
    "        \n",
    "        inicio = \"1.0+\" + str(match.start()) + \"c\"\n",
    "        fin = \"1.0+\" + str(match.end()) + \"c\"\n",
    "        \n",
    "        # Agregar etiqueta de enlace al patrón de URL encontrado\n",
    "        chat_area.tag_add(\"url\" + str(i), inicio, fin)\n",
    "        \n",
    "        # Configurar el cursor como \"mano\" cuando el mouse pasa sobre el enlace\n",
    "        chat_area.tag_bind(\"url\" + str(i), \"<Enter>\", lambda event: chat_area.config(cursor=\"hand2\"))\n",
    "        chat_area.tag_bind(\"url\" + str(i), \"<Leave>\", lambda event: chat_area.config(cursor=\"\"))\n",
    "        \n",
    "        # Configurar el enlace\n",
    "        url = match.group()\n",
    "        chat_area.tag_bind(\"url\" + str(i), \"<Button-1>\", lambda event, url=url: open_url(url))\n",
    "    \n",
    "def open_url(url):\n",
    "    webbrowser.open_new(url)\n",
    "\n",
    "# Sirve para ver si la tecla presionada es Enter. En ese caso, envía el mensaje.\n",
    "def on_key_pressed(event):\n",
    "    key = event.keysym\n",
    "    \n",
    "    if key == \"Return\":\n",
    "        change_focus()\n",
    "        send_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a050649",
   "metadata": {},
   "source": [
    "### Función `create_images()`:\n",
    "\n",
    "Lo que hace esta función es crear elementos de la GUI con las imágenes que representan al sistema y al usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7941550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_images():\n",
    "    global jarvis_icon\n",
    "    jarvis_icon = tk.PhotoImage(file=\"GUI/antOutline-robot 1@1x.png\")\n",
    "    \n",
    "    global user_icon\n",
    "    user_icon = tk.PhotoImage(file=\"GUI/md-person_outline 1@1x.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0ea2b",
   "metadata": {},
   "source": [
    "### Creación de la GUI:\n",
    "\n",
    "El código a continuación se encarga de crear la GUI con la que interactua el usuario para hacer uso de la aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ccc4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-47:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\usuario\\anaconda3\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\usuario\\anaconda3\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\usuario\\AppData\\Local\\Temp\\ipykernel_1580\\408745058.py\", line 5, in begin_voice_chat\n",
      "  File \"C:\\Users\\usuario\\AppData\\Local\\Temp\\ipykernel_1580\\2106399524.py\", line 37, in begin_streaming\n",
      "  File \"C:\\Users\\usuario\\AppData\\Local\\Temp\\ipykernel_1580\\2106399524.py\", line 83, in listen_print_loop\n",
      "  File \"C:\\Users\\usuario\\anaconda3\\lib\\tkinter\\__init__.py\", line 3602, in delete\n",
      "    self.tk.call(self._w, 'delete', index1, index2)\n",
      "RuntimeError: main thread is not in main loop\n"
     ]
    }
   ],
   "source": [
    "# GUI.\n",
    "# Creaciónde una ventana.\n",
    "window = tk.Tk()\n",
    "window.geometry(\"1080x763\")\n",
    "window.title(\"Jarvis\")\n",
    "window.configure(background='#333333')\n",
    "\n",
    "# Crear el botón con una imagen\n",
    "photo = tk.PhotoImage(file=\"GUI/md-send 1@1x.png\")\n",
    "send_bttn = tk.Button(window, image=photo)\n",
    "\n",
    "send_bttn.place(x=864, y=655, width=108, height=108)\n",
    "send_bttn.configure(background='#333333', borderwidth=0, highlightthickness=0, cursor=\"hand2\")\n",
    "\n",
    "send_bttn.bind(\"<Enter>\", hover_on_send)\n",
    "send_bttn.bind(\"<Leave>\", hover_off_send)\n",
    "send_bttn.bind(\"<Button-1>\", on_click_send_bttn)\n",
    "\n",
    "# Crear el botón con una imagen\n",
    "photo_talk = tk.PhotoImage(file=\"GUI/md-mic_none 1@1x.png\")\n",
    "talk_bttn = tk.Button(window, image=photo_talk)\n",
    "\n",
    "talk_bttn.place(x=972, y=655, width=108, height=108)\n",
    "talk_bttn.configure(background='#333333', borderwidth=0, highlightthickness=0, cursor=\"hand2\")\n",
    "\n",
    "talk_bttn.bind(\"<Enter>\", hover_on_talk)\n",
    "talk_bttn.bind(\"<Leave>\", hover_off_talk)\n",
    "talk_bttn.bind(\"<Button-1>\", on_click_talk_bttn)\n",
    "\n",
    "# Crear textArea.\n",
    "message_text = tk.Text(window, wrap=\"word\")\n",
    "message_text.place(x=5, y=677, height=63, width=853)\n",
    "message_text.configure(font=('Arial',16), fg='grey')\n",
    "\n",
    "message_text.insert(\"1.0\", 'Digita tus palabras para Jarvis...')\n",
    "\n",
    "message_text.bind(\"<FocusIn>\", hide_prompt)\n",
    "message_text.bind(\"<FocusOut>\", show_prompt)\n",
    "message_text.bind(\"<Key>\", on_key_pressed)\n",
    "\n",
    "# Crear ChatArea.\n",
    "chat_area = tk.Text(window, wrap=\"word\")\n",
    "chat_area.place(x=3, y=3, height=652, width=1074)\n",
    "chat_area.configure(font=('Arial',16), fg='white', background='#333333', border=0, state='disabled', \n",
    "                    borderwidth=0, highlightthickness=1, highlightcolor='white')\n",
    "\n",
    "jarvis_icon = None\n",
    "user_icon = None\n",
    "create_images()\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ee3ec",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
